{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align='center'> <font color='red'> Bolum 8- Karar Destek Makinalari\n",
    "# <div align='center'> <font color='red'>    ODEV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "İkinci proje olan regresyon projesinde ve üçüncü proje olan sınıflandırma projesinde kullandığınız veri kümelerini tekrar kullanarak, SVM ile regresyon ve sınıflandırma modelleri oluşturun ve bu modeller için en uygun parametre değerlerini belirleyin. En ideal parametreyi bulmak için daha önceki bölümde öğrendiğiniz çapraz doğrulama metotlarını kullanabilirsiniz.\n",
    "\n",
    "\n",
    "Makine öğrenmesi kendi içerisinde 3'e Ayrılır. Bunlar;\n",
    "\n",
    "* **Gözetimli Öğrenme(Supervised Learning):** Gözetimli öğrenmede hedef değişken bellidir, tahmin edilmek istenen sınıflar bellidir, buradaki amaç girdi değerleri ile hedef değişken arasında bir bağlamı öğrenerek yeni gelen değerlerde bu bağlamdan yola çıkarak tahminler yapmaktır. Gözetimli Öğrenme ile Sınıflandırma, Regresyon Yapabilirsiniz.\n",
    "\n",
    "* **Gözetimsiz Öğrenme(Unsupervised Learning):** Gözetimsiz öğrenmede ise hedef değişken bulunmamaktadır. Elimizde sadece girdi bulunmaktadır. Amaç bu girdiler arasındaki yakınlıklar, düzenlilikleri bulmaktır. Gözetimsiz öğrenme ile ise kümeleme ve kestirim yapabilirsiniz.\n",
    "\n",
    "* **Takviyeli Öğrenme(Reinforcement Learning):** Takviyeli öğrenme ise gözetimli/gözetimsiz öğrenmeden çok daha ayrı, farklı bir yöntem ile çalışır. Burada bir ödül-ceza sistemi bulunmaktadır. Burada makinen amacı istenilen eyleme ulaşılan doğru yolu bulmaktır, doğru yola giderken yaptığı hatalardan çıkarımlar yapar ve belli bir ödül-ceza temeli üzerinde çalışır. Ardından çıkarımlarından en optimize yol (en az hata) ile doğru eylemi bulmaya çalışır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Hilesi\n",
    "\n",
    "Çok değişkenli gerçek veriler ile SVM çalıştırmak istediğimizde, çok büyük sayıda özellik çok yüksek boyutta uzaya neden olacaktır. Bu şekilde verileri dönüştürmeye çalışmak, pratikte daha çok işlemci ve daha çok hafıza kaynağı gerektirecektir. Gerçek veriler ile çalıştığımızda bu durum, dönüşümü yapmamızı imkansız hale getirecektir.\n",
    "\n",
    "Peki verilerimizi yüksek boyutlu uzaya çevirmeden yüksek boyutlu optimum hiperdüzlemi bulabilir miyiz? Evet bunu Kernel Hilesi adı verilen işlem ile yapabiliriz. Kernel hilesi araştırmamızı yüksek boyutlu hiper düzlem sınırlarındaki pratik imkansızlıklardan çok daha makul işlem gerektiren bir göreve dönüştürür. \n",
    "\n",
    "What **Kernel Trick** does is it utilizes existing features, applies some transformations, and creates new features. Those new features are the key for SVM to find the nonlinear decision boundary.\n",
    "https://towardsdatascience.com/support-vector-machine-simply-explained-fee28eba5496"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel = RBF\n",
    "\n",
    "This kernel transformation strategy is used often in machine learning to turn fast linear methods into fast nonlinear methods, especially for models in which the kernel trick can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/wine.csv')\n",
    "df1.head()\n",
    "\n",
    "X = df1.drop(['quality_range'], axis=1)\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "y = df1['quality_range']\n",
    "\n",
    "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.20, random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='green'> <div align = 'center'>Choosing The Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.46\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "decisiontree = DecisionTreeClassifier(max_depth = 5)\n",
    "decisiontree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = decisiontree.predict(X_test)\n",
    "\n",
    "acc_decisiontree = round(accuracy_score(y_pred, y_test) * 100, 2)\n",
    "print(acc_decisiontree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.54\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "randomforest = RandomForestClassifier()\n",
    "randomforest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = randomforest.predict(X_test)\n",
    "\n",
    "acc_randomforest = round(accuracy_score(y_pred, y_test) * 100, 2)\n",
    "print(acc_randomforest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "acc_svc = round(accuracy_score(y_pred, y_test) * 100, 2)\n",
    "print(acc_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.0\n"
     ]
    }
   ],
   "source": [
    "#SVC((support vector classifier))\n",
    "\n",
    "#C=1-->Default Penalty/Default Tolerance\n",
    "#C=0.01-->Less Penalty/More Tolearance\n",
    "\n",
    "svc = SVC(kernel='linear', C=1) #C is a valuation of \"how badly\" you want to properly classify, or fit, everything. \n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "acc_svc_linear = round(accuracy_score(y_pred,y_test)*100, 2)\n",
    "print(acc_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "acc_knn = round(accuracy_score(y_pred, y_test) * 100, 2)\n",
    "print(acc_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.15\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "acc_logreg = round(accuracy_score(y_pred, y_test) * 100, 2)\n",
    "print(acc_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4401375948626393\n"
     ]
    }
   ],
   "source": [
    "#SVM(Support Vector Machine) ile Regresyon\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "svr_reg = SVR() \n",
    "svr_reg.fit(X_train, y_train)\n",
    "\n",
    "y_tahmin = svr_reg.predict(X_test)\n",
    "\n",
    "rmse_test = MSE(y_test, y_tahmin)**(1/2)\n",
    "print(rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>82.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Desicion Tree</td>\n",
       "      <td>73.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>73.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC Linear</td>\n",
       "      <td>72.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Knn</td>\n",
       "      <td>68.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVR REG</td>\n",
       "      <td>0.440138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model      Score\n",
       "1        Random Forest  82.540000\n",
       "0        Desicion Tree  73.460000\n",
       "5  Logistic Regression  73.150000\n",
       "3           SVC Linear  72.310000\n",
       "2                  SVC  70.000000\n",
       "4                  Knn  68.920000\n",
       "6              SVR REG   0.440138"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pd.DataFrame({'Model':['Desicion Tree', 'Random Forest', 'SVC','SVC Linear', 'Knn', 'Logistic Regression', 'SVR REG'],\n",
    "                       'Score':[acc_decisiontree, acc_randomforest, acc_svc, acc_svc_linear, acc_knn, acc_logreg, rmse_test]})\n",
    "\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def plotSVC(title):\n",
    "    # create a mesh to plot in\n",
    "    #x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "   # y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "   # h = (x_max / x_min)/100\n",
    "   # xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "   # np.arange(y_min, y_max, h))\n",
    "\n",
    "   # Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "   # Z = Z.reshape(xx.shape)\n",
    "   # plt.contourf(xx, yy, Z, cmap='viridis', alpha=0.8)\n",
    "\n",
    "  #  plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')\n",
    "  #  plt.xlabel('Taç Yaprak Uzunluğu')\n",
    "   # plt.ylabel('Taç Yaprak Genişliği')\n",
    "   # plt.xlim(xx.min(), xx.max())\n",
    "   # plt.title(title, color='darkred')\n",
    "   # return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernel_listesi = ['linear', 'rbf', 'poly']\n",
    "\n",
    "plt.figure(figsize=(15,4), dpi=100)\n",
    "for i, kernel in enumerate(kernel_listesi):\n",
    "    svc = SVC(kernel=kernel).fit(X, y) #SVM((support vector machine))\n",
    "    plt.subplot(1,3,i+1)\n",
    "   # plotSVC('kernel=' + str(kernel)) ----> plotSVC olusturamadim\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_listesi = [0.1, 1, 10]\n",
    "\n",
    "\n",
    "for i, gamma in enumerate(gamma_listesi):\n",
    "    svc = SVC(kernel='rbf', gamma=gamma).fit(X, y)\n",
    "    svc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-876e2259276d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msvc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rbf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msvc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0my_tahmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: fit() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "c_listesi = [0.1, 1, 10]\n",
    "   \n",
    "\n",
    "for i, c in enumerate(c_listesi):\n",
    "    svc = SVC(kernel='rbf', C=c).fit(X_train, y_train)\n",
    "    y_tahmin=svc.fit(X_test)\n",
    "    score = svc.score(X, y)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "print('Highest Accuracy Score: ', best_score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for C in np.arange(0.05, 2.05, 0.05):\n",
    "    for gamma in np.arange(0.001, 0.101, 0.001):\n",
    "        svc = SVC(kernel='rbf', gamma=gamma, C=C)\n",
    "        svc.fit(X_eğitim, y_eğitim)\n",
    "        score = svc.score(X_test, y_test)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "print('Highest Accuracy Score: ', best_score) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degree Listeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_listesi = [1, 2, 3, 4, 5, 6]\n",
    "   \n",
    "plt.figure(figsize=(15,10))\n",
    "for i, degree in enumerate(degree_listesi):\n",
    "    svc = SVC(kernel='poly', degree=degree).fit(X, y)\n",
    "    svc.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
